{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Basic Idea of GAN \n",
    "This is where the term *adversarial* comes from.\n",
    "\n",
    "Student:\n",
    "NN Generator v1 -> NN Generator v2 -> NN Generator v3 ->  \n",
    "$\\downarrow$    \n",
    "Teacher :\n",
    "Discriminator v1 ->  Discriminator v2 ->  Discriminator v3 ->\n",
    "\n",
    "---\n",
    "### Algorithm\n",
    "\n",
    "#### Stepwize\n",
    "- Step 1: \n",
    "\t- Fix generator G, and update discriminator D\n",
    "\t- Discriminator learns to assign high scores to real objects and low scores to generated objects\n",
    "\n",
    "- Step 2:\n",
    " \t- Fix discriminator D, and update generator G\n",
    " \t- Generator learns to *fool* the discriminator\n",
    " \t- G & D is a huge NN, but there is one hidden layer(output/image size) in between\n",
    "\n",
    "#### Pesudo\n",
    "-  Initialize $\\theta_d$ for D and $\\theta_g$ for G\n",
    "-  In each training iteration\n",
    "\t-  Learning **D**\n",
    "\t\t-  Sample $m$ examples $\\{x^1, x^2,...,x^m\\}$ from database\n",
    "\t\t-  Sample $m$ noise samples $\\{z^1, z^2,...,z^m\\}$ from a distribution (usually Gaussian)\n",
    "\t\t-  Obtaining generated data $\\{\\widetilde{x}^1,\\widetilde{x}^2,...,\\widetilde{x}^m\\}$, $\\widetilde{x}^i = G(z^i)$\n",
    "\t\t- Update discriminator parameters $\\theta_d$ to maximize\n",
    "\t\t\n",
    "\t\t\t| $\\widetilde{V} = \\frac{1}{m}\\sum^m_{i=1}logD(x^i) + \\frac{1}{m}\\sum^m_{i=1}log(1-D(\\tilde x^i))$|\n",
    "\t\t\t|---|\n",
    "\t\t\t|$\\theta_d \\leftarrow \\theta_d + \\eta\\nabla\\widetilde{V}(\\theta_d)$|\n",
    "\n",
    "\t- Learning **G**\n",
    "\t\t- Sample $m$ noise samples $\\{z^1, z^2,...,z^m\\}$ from a distribution\n",
    "\t\t- Update generator parameters $\\theta_g$ to maximize\n",
    "\t\t\t\n",
    "\t\t\t| $\\widetilde{V} = \\frac{1}{m}\\sum^m_{i=1}log(D(G(x^i)))$|\n",
    "\t\t\t|---|\n",
    "\t\t\t|$\\theta_g \\leftarrow \\theta_g + \\eta\\nabla\\widetilde{V}(\\theta_g)$|\n",
    "---\n",
    "## GAN as structured learning\n",
    "- recall:\n",
    "\t- Regression: output a scalar\n",
    "\t- Classification: output a \"class\" (one-hot vector)\n",
    "\t- Structured Leraning/ Prediction: output a sequence, a matrix, a graph, a tree\n",
    "\t\t- ex: Machine Translation, Speech Recognition, Chat-bot...\n",
    "\n",
    "\n",
    "- Why structured learning challenging?\n",
    "\t- One-shot/zero-shot learning\n",
    "\t\t- Classification -> each class has examples\n",
    "\t\t- Structured learning -> each posiible output is a class, need **creation**\n",
    "\t- Machine has to learn to do planning\n",
    "\t\t- relation between components\n",
    "\n",
    "\n",
    "- Structured Learning Approach\n",
    "\t - Bottom Up + Top Down -> GAN\n",
    "\t\t- Generative: Bottom Up \n",
    "\t\t\t -  Learn to generate the object at component level\n",
    "\t\t- Discriminator: Top Down\n",
    "\t\t\t- Evaluating the whole object, and find the best one\n",
    "\t\t\n",
    "---\n",
    "## Can Generator learn by itself?\n",
    "\n",
    "\t \n",
    "\n",
    "Generate can learn itself\n",
    "\n",
    "Input a vector\n",
    "\n",
    "Vector -> NN Generator -> image <=> target image as close as possible\n",
    "\n",
    "c.f\n",
    "training image -> NN Classifier -> Vector <=> one-hot vector\n",
    "\n",
    "Use encoder to extract characteristics: Auto-encoder\n",
    "\n",
    "image -> NN Encoder -> code -> NN Decoder-> image (as close as possible)\n",
    "\n",
    "Use Variational auto-encoder instead\n",
    "\n",
    "---\n",
    "\n",
    "### What do we miss\n",
    "Meaning of _As close as posiible_\n",
    "Two impage of Ecudien distance\n",
    "\n",
    "The relation between the components are critical. Although highly correlated, they cannot influence each other.\n",
    "\n",
    "Need deep structure to catch the relation between components.\n",
    "\n",
    "Can Discriminator generate? Not really\n",
    "\n",
    "Discriminator\n",
    "\n",
    "Discriminator is a function D (network, can deep)\n",
    ": Evaluation function, potentail function, energy function => output scalar\n",
    "\n",
    "Can we use the to generater objects? Yes\n",
    "\n",
    "It is easier to catch the relation between the components by top-down evalutation.\n",
    "\n",
    "\n",
    "Suppose we alreafy have a good discriminator D(x)\n",
    "$\\widetilde{x} = arg \\max_{x\\in X}D(x)$\n",
    "\n",
    "Enumerate all possible x ??? How to learn the discriminator?\n",
    "\n",
    "Discriminator training needs some _Good_ negative example.\n",
    "\n",
    "How to generate realistic negative examples?\n",
    "\n",
    "- General Algorithm\n",
    "\t- Given a set of _positive examples_, randomly generate a set of _negative examples_\n",
    "\t- In each iteration\n",
    "\t\t- Learn a discriminator D that can discriminate positive and negative examples.\n",
    "\t\t- Generate negative examples by discriminator D\n",
    "\t\t\t- $\\widetilde{x} = arg \\max_{x\\in X}D(x)$\n",
    "\n",
    "---\n",
    "\n",
    "### Generator v.s. Discriminator\n",
    "||Pros|Cons|\n",
    "|---|---|---|\n",
    "|Generator|Easy to generate even with deep model|<ul><li>Imitate the apperance</li><li>Hard to learn the correlation between components</li></ul> |\n",
    "|Discriminator|Consideraing the big picture|<ul><li>Generation is not alsways feasible(Epecially when your model is deep)</li><li>How to do nagative sampling</li></ul> |\n",
    "\n",
    "### Generator + Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
